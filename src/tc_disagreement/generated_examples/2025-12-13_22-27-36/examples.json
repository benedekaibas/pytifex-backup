{
    "timestamp": "2025-12-13T22:27:36.494226",
    "model_used": "gemini-2.5-flash",
    "raw_response": "Here are 10 Python code snippets demonstrating known divergences between mypy, pyright, pyre, and zuban type checkers, adhering to all critical requirements.\n\n---\n\n### Snippet 1: Protocol with Default Arguments (Absence of Default)\n\n```python\n# id: protocol-default-arg-absence\n# EXPECTED:\n#   mypy: No error. Mypy considers `path: str` compatible with `path: str = ...` in an implementation for a protocol.\n#   pyright: No error. Pyright typically allows this as long as the implementation's argument types and positions are compatible with all possible calls allowed by the protocol.\n#   pyre: Error. Pyre is often stricter, flagging an incompatibility because `FileConfigLoader.load` lacks a default argument, while `ConfigLoader.load` specifies one, which would make a call like `loader.load()` ambiguous for `FileConfigLoader`.\n#   zuban: No error. Focuses on call signature compatibility (i.e., whether the implementation can satisfy all calls).\n# REASON: Type checkers differ on whether the presence or absence of a default argument in an implementation, when the protocol specifies one, constitutes a signature mismatch. Some prioritize callable signature compatibility (parameters, types), allowing an implementation to omit a default if it can still satisfy calls where the argument is either provided or expected to be optional. Others view the default argument as part of the callable's contract that must be present.\nfrom typing import Protocol\n\nclass ConfigLoader(Protocol):\n    \"\"\"Protocol requiring a load method with an optional path.\"\"\"\n    def load(self, path: str = \"default.ini\") -> dict[str, str]: ...\n\nclass FileConfigLoader:\n    \"\"\"Implementation that omits the default argument.\"\"\"\n    def load(self, path: str) -> dict[str, str]: # Protocol specifies default, implementation does not\n        print(f\"FileConfigLoader: Loading from {path}\")\n        return {\"file_key\": \"file_value\"}\n\nclass DefaultConfigLoader:\n    \"\"\"Implementation with a different default argument.\"\"\"\n    def load(self, path: str = \"another.cfg\") -> dict[str, str]: # Protocol specifies default, implementation has different default\n        print(f\"DefaultConfigLoader: Loading from {path} with different default\")\n        return {\"default_key\": \"default_value\"}\n\ndef process_config(loader: ConfigLoader) -> None:\n    # This call relies on the protocol's default argument\n    # Pyre is expected to flag an error here if `loader` is `FileConfigLoader` because `FileConfigLoader.load` has no default.\n    config = loader.load()\n    print(f\"    Processed config (default): {config}\")\n\n    # This call explicitly provides an argument\n    config = loader.load(\"custom.json\")\n    print(f\"    Processed config (custom): {config}\")\n\nif __name__ == \"__main__\":\n    print(\"--- Testing FileConfigLoader ---\")\n    file_loader: ConfigLoader = FileConfigLoader() # Potential divergence point\n    process_config(file_loader)\n    \n    print(\"\\n--- Testing DefaultConfigLoader ---\")\n    default_loader: ConfigLoader = DefaultConfigLoader() # All should typically allow\n    process_config(default_loader)\n```\n\n---\n\n### Snippet 2: TypeGuard with Generic Narrowing\n\n```python\n# id: typeguard-generic-narrowing\n# EXPECTED:\n#   mypy: No error. Mypy successfully narrows `data` to `list[int]` within the `if` block. `reveal_type(data)` -> `list[int]`.\n#   pyright: No error. Pyright is very strong with TypeGuard and generic narrowing. `reveal_type(data)` -> `list[int]`.\n#   pyre: Error on `data.append(\"string\")`. Pyre might not fully narrow `data` to `list[int]`, potentially leaving it as `list[Union[int, str]]` or a less specific type, thus allowing a `str` to be appended even after the guard. Or it might be conservative about the narrowing itself.\n#   zuban: No error. Aims for precise TypeGuard application.\n# REASON: Type checkers differ in their ability to perform precise generic narrowing, especially when a TypeGuard makes assertions about the elements of a generic container. Some might not fully propagate the narrowing to the container's type parameter, leading to issues with subsequent operations that should be restricted by the narrowed type.\nfrom typing import TypeGuard, TypeVar, List, Union\n\nT = TypeVar('T')\n\ndef is_all_type(val: List[object], target_type: type[T]) -> TypeGuard[List[T]]:\n    \"\"\"TypeGuard that checks if all elements in the list are of a specific type.\"\"\"\n    return all(isinstance(x, target_type) for x in val)\n\ndef process_items(data: List[Union[int, str, bool]]) -> None:\n    print(f\"Initial data: {data}\")\n\n    if is_all_type(data, int):\n        print(\"  Inside 'is_all_type(int)' block.\")\n        # Inside this block, 'data' should be narrowed to 'list[int]'\n        data.append(100) # This should be allowed by all\n        reveal_type(data) # Expected: list[int]\n\n        # Pyre might flag this if it didn't fully narrow, incorrectly thinking 'str' is still possible.\n        # Or, conversely, if it fails to narrow, it might *not* flag data.append(\"string\") below.\n        # The divergence is often in what *is* allowed/disallowed due to narrowing.\n        print(f\"  Data after appending int: {data}\")\n\n    print(\"  Outside 'is_all_type(int)' block.\")\n    # Outside the if block, data is still List[Union[int, str, bool]]\n    # This should be allowed by all.\n    data.append(\"new string\")\n    print(f\"  Data after appending str: {data}\")\n\n    # This should always be an error, but if Pyre *fails* to narrow,\n    # it might allow appending `str` inside the `if` block, which would be a divergence.\n    # The snippet demonstrates a valid operation (append int) when narrowed,\n    # expecting Pyre to potentially fail on the type assertion itself,\n    # or fail to narrow so that a later invalid append is not caught.\n    # Let's add an *explicitly wrong* append for narrowed type.\n    if is_all_type(data, str):\n        data.append(123) # This should be an error for all, but TypeGuard itself is the focus.\n\nif __name__ == \"__main__\":\n    my_mixed_list: List[Union[int, str, bool]] = [1, 2, \"three\", True, 4]\n    process_items(my_mixed_list)\n\n    print(\"\\n--- Processing all integers list ---\")\n    my_int_list: List[Union[int, str, bool]] = [10, 20, 30]\n    process_items(my_int_list)\n```\n\n---\n\n### Snippet 3: TypedDict with `Required`/`NotRequired` and `total=False` mixed semantics\n\n```python\n# id: typeddict-total-mixed-required-notrequired\n# EXPECTED:\n#   mypy: No error. All `reveal_type`s should be `Union[Type, None]` for `.get()`. `total=False` propagates.\n#   pyright: No error. All `reveal_type`s should be `Type | None` for `.get()`. `Required` overrides `total=False` for specific keys.\n#   pyre: Subtle error/difference. Pyre might treat 'optional_param' as strictly missing if `total=False` isn't fully propagated for inherited keys, leading to an error on `td.get('optional_param')` or incorrect `reveal_type`. Or it might raise an error during assignment if it misinterprets the optionality.\n#   zuban: No error. Aims for precise TypedDict semantics.\n# REASON: The interaction between `total=False` on a base `TypedDict` and explicit `Required`/`NotRequired` in a derived one can lead to different interpretations of key optionality. This divergence is often seen in how `.get()` is typed for keys whose optionality is implicitly inherited versus explicitly defined, or in how `total=False` inheritance affects new keys.\nfrom typing import TypedDict, Union\nfrom typing_extensions import Required, NotRequired\n\nclass BaseOpts(TypedDict, total=False):\n    \"\"\"Base TypedDict where all keys are optional by default.\"\"\"\n    config_id: int\n    log_file: str\n\nclass AdvancedOpts(BaseOpts):\n    \"\"\"Derived TypedDict, mixing explicit optionality and inheriting total=False.\"\"\"\n    # new_param is implicitly NotRequired due to inheriting total=False\n    new_param: float\n    \n    # Required keys explicitly override total=False for themselves\n    api_key: Required[str]\n    \n    # Explicitly NotRequired (redundant given total=False, but allowed)\n    debug_mode: NotRequired[bool]\n\ndef process_options(opts: AdvancedOpts) -> None:\n    # 'api_key' is Required, but .get() always returns Optional.\n    api_k = opts.get('api_key')\n    reveal_type(api_k) # EXPECTED: str | None (all should agree)\n    if api_k:\n        print(f\"API Key: {api_k}\")\n\n    # 'config_id' is implicitly NotRequired from BaseOpts(total=False)\n    cfg_id = opts.get('config_id')\n    reveal_type(cfg_id) # EXPECTED: int | None (pyre might differ here or on the call)\n    if cfg_id is not None:\n        print(f\"Config ID: {cfg_id}\")\n\n    # 'new_param' is implicitly NotRequired as it's a new key in a total=False derived TypedDict.\n    new_p = opts.get('new_param')\n    reveal_type(new_p) # EXPECTED: float | None (pyre might differ here)\n    if new_p is not None:\n        print(f\"New Param: {new_p}\")\n\n    # 'debug_mode' is explicitly NotRequired.\n    dbg_m = opts.get('debug_mode')\n    reveal_type(dbg_m) # EXPECTED: bool | None (all should agree)\n    if dbg_m is not None:\n        print(f\"Debug Mode: {dbg_m}\")\n\nif __name__ == \"__main__\":\n    # Valid config: includes 'api_key' (Required), others are optional\n    config1: AdvancedOpts = {'api_key': 'abc-123', 'config_id': 101, 'new_param': 3.14}\n    print(\"--- Processing config1 ---\")\n    process_options(config1)\n\n    # Config with only required key\n    config2: AdvancedOpts = {'api_key': 'xyz-456'}\n    print(\"\\n--- Processing config2 ---\")\n    process_options(config2)\n\n    # This assignment would fail for all checkers if 'api_key' is missing.\n    # config3: AdvancedOpts = {'config_id': 202}\n```\n\n---\n\n### Snippet 4: `ParamSpec` with Decorators on Class Methods\n\n```python\n# id: paramspec-classmethod-decorator-signature\n# EXPECTED:\n#   mypy: Error. Mypy has historically struggled with ParamSpec on classmethod/staticmethod, often losing the `cls` argument or misinterpreting the signature, leading to an error when calling `Service.create_named_resource`.\n#   pyright: No error. Pyright is generally very robust with ParamSpec, including on class methods, correctly inferring the `cls` argument's behavior.\n#   pyre: Error. Similar to mypy's historical issues, Pyre might misinterpret the signature after decoration.\n#   zuban: No error. Aims for high compatibility with modern Python typing features.\n# REASON: Type checkers differ in their ability to correctly infer and preserve the signature of class methods when they are decorated by a `ParamSpec`-aware decorator. The implicit `cls` (or `self`) argument can be mishandled, leading to incorrect call signatures or type errors when the method is invoked, as `ParamSpec` needs to correctly capture or omit it.\nfrom typing import TypeVar, Callable, ParamSpec, Type\nfrom typing_extensions import Concatenate # Explicitly show usage though not strictly needed for this example\n\nP = ParamSpec('P')\nR = TypeVar('R')\nClsT = TypeVar('ClsT') # Type variable for the class type in classmethods\n\ndef log_and_return(func: Callable[P, R]) -> Callable[P, R]:\n    \"\"\"A simple decorator that logs and returns the result.\"\"\"\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:\n        print(f\"LOG: Calling '{func.__name__}' with args={args}, kwargs={kwargs}\")\n        result = func(*args, **kwargs)\n        print(f\"LOG: '{func.__name__}' returned: {result}\")\n        return result\n    return wrapper\n\nclass Resource:\n    _registry: dict[str, 'Resource'] = {}\n\n    def __init__(self, name: str, value: int) -> None:\n        self.name = name\n        self.value = value\n        self.__class__._registry[name] = self\n\n    @log_and_return\n    @classmethod\n    def create_named_resource(cls: Type[ClsT], name: str, initial_value: int = 0) -> ClsT:\n        \"\"\"A class method to create or retrieve a resource.\"\"\"\n        if name not in cls._registry:\n            print(f\"  Creating new resource: {name}\")\n            return cls(name, initial_value)\n        print(f\"  Retrieving existing resource: {name}\")\n        return cls._registry[name] # type: ignore [return-value] # This ignore is for runtime, type checkers handle it\n\nif __name__ == \"__main__\":\n    print(\"--- Calling create_named_resource ---\")\n    # This call should be type-checked based on `create_named_resource`'s\n    # original signature, correctly handling `cls` and `name: str, initial_value: int`.\n    resource1 = Resource.create_named_resource(\"Alpha\", 10)\n    print(f\"Resource 1: {resource1.name}, {resource1.value}\")\n    reveal_type(resource1) # EXPECTED: Resource\n\n    resource2 = Resource.create_named_resource(\"Beta\") # Uses default initial_value\n    print(f\"Resource 2: {resource2.name}, {resource2.value}\")\n\n    # The actual divergence will be seen here:\n    # Mypy/Pyre might reveal a signature that doesn't include 'initial_value' or misattributes 'name'\n    reveal_type(Resource.create_named_resource) # Check the inferred signature\n```\n\n---\n\n### Snippet 5: `Self` in Generics with Abstract Methods\n\n```python\n# id: self-in-generic-abstract-method\n# EXPECTED:\n#   mypy: Error on `cloned_processor: StringProcessor = processor_instance.duplicate()`. Mypy might resolve `Self` in the generic context to `BaseProcessor[T]` instead of the specific concrete subclass (`StringProcessor`). `reveal_type(cloned_processor_var)` would show `BaseProcessor[str]`.\n#   pyright: No error. Pyright generally handles `Self` correctly even within generic abstract methods, inferring the concrete class type. `reveal_type(cloned_processor_var)` -> `StringProcessor`.\n#   pyre: Error. Pyre often struggles with complex `Self` and generics interactions, similar to mypy.\n#   zuban: No error. Aims for precise type inference for `Self`.\n# REASON: Type checkers differ in how they resolve the `Self` type when it's used as a return type in an abstract method within a generic class. Some might incorrectly resolve `Self` to the generic base class (e.g., `BaseProcessor[str]`), rather than the specific concrete subclass (`StringProcessor`), leading to type mismatches during assignment or further operations.\nfrom abc import ABC, abstractmethod\nfrom typing import Generic, TypeVar, Type\nfrom typing_extensions import Self\n\nT = TypeVar('T')\n\nclass BaseProcessor(ABC, Generic[T]):\n    \"\"\"Abstract generic class for processing data.\"\"\"\n    def __init__(self, data: T) -> None:\n        self._data = data\n\n    @abstractmethod\n    def process(self) -> T:\n        \"\"\"Processes the internal data.\"\"\"\n        ...\n\n    @abstractmethod\n    def duplicate(self) -> Self:\n        \"\"\"Returns a new instance of the concrete processor type, using Self.\"\"\"\n        # The critical point: Self should resolve to the implementing class.\n        ...\n\nclass StringProcessor(BaseProcessor[str]):\n    \"\"\"Concrete processor for string data.\"\"\"\n    def process(self) -> str:\n        return f\"Processed: {self._data.upper()}\"\n\n    def duplicate(self) -> Self:\n        # `Self` should resolve to `StringProcessor` here.\n        # This factory call should return an instance of `StringProcessor`.\n        print(f\"Duplicating {type(self).__name__} with data '{self._data}'\")\n        return type(self)(self._data + \" (copy)\")\n\nclass IntProcessor(BaseProcessor[int]):\n    \"\"\"Concrete processor for integer data.\"\"\"\n    def process(self) -> int:\n        return self._data * 2\n\n    def duplicate(self) -> Self:\n        print(f\"Duplicating {type(self).__name__} with data '{self._data}'\")\n        return type(self)(self._data + 10)\n\n\ndef check_processor(processor: BaseProcessor[str]) -> BaseProcessor[str]:\n    # This call demonstrates that `duplicate` *can* be called via the base type.\n    # The return type should still be compatible with the base generic.\n    return processor.duplicate()\n\nif __name__ == \"__main__\":\n    processor_instance = StringProcessor(\"hello\")\n    print(f\"Original type: {type(processor_instance).__name__}\")\n    \n    # This assignment is the key divergence point.\n    # If Self resolves correctly, this is fine. If not, mypy/pyre will error.\n    cloned_processor: StringProcessor = processor_instance.duplicate()\n    \n    print(f\"Cloned type: {type(cloned_processor).__name__}\")\n    print(f\"Cloned data: {cloned_processor._data}\")\n    reveal_type(cloned_processor) # EXPECTED: StringProcessor\n\n    print(\"\\n--- Testing IntProcessor ---\")\n    int_processor_instance = IntProcessor(10)\n    cloned_int_processor: IntProcessor = int_processor_instance.duplicate()\n    print(f\"Cloned int processor type: {type(cloned_int_processor).__name__}\")\n    reveal_type(cloned_int_processor) # EXPECTED: IntProcessor\n\n    print(\"\\n--- Testing through base type hint ---\")\n    base_processor_ref: BaseProcessor[str] = StringProcessor(\"base_test\")\n    returned_base_processor = check_processor(base_processor_ref)\n    reveal_type(returned_base_processor) # EXPECTED: BaseProcessor[str]\n```\n\n---\n\n### Snippet 6: `NewType` and `List` Contravariance\n\n```python\n# id: newtype-list-contravariance\n# EXPECTED:\n#   mypy: Error on `return get_raw_ids()`. Mypy correctly identifies that `List[int]` cannot be assigned to `List[TransactionId]` due to NewType's strictness and list invariance (for assignment), even though `int` is the base type of `TransactionId`.\n#   pyright: Error on `return get_raw_ids()`. Pyright also correctly enforces NewType and list type compatibility.\n#   pyre: No error. Pyre has historically been observed to be less strict about `NewType` in collection contexts, potentially allowing `List[int]` to be compatible with `List[TransactionId]` in return positions.\n#   zuban: Error. Aims for strict type safety with NewType.\n# REASON: Type checkers differ in their strictness when a `NewType` (which is a nominal subtype) is involved in generic collections. While `NewType` is a subtype of its base type (`int`), a collection of the base type (`List[int]`) is not type-compatible with a collection of the `NewType` (`List[TransactionId]`). This is a subtle contravariance/invariance rule that some checkers might relax.\nfrom typing import NewType, List, Callable, TypeVar, Any\n\nTransactionId = NewType('TransactionId', int)\nItemId = NewType('ItemId', int)\n\ndef get_raw_ids() -> List[int]:\n    \"\"\"Simulates fetching raw integer IDs from a database.\"\"\"\n    print(\"Fetching raw integer IDs.\")\n    return [101, 102, 103, 104]\n\ndef fetch_transaction_ids() -> List[TransactionId]:\n    \"\"\"Function expected to return a list of TransactionId NewType.\"\"\"\n    # This line is the potential divergence point.\n    # It attempts to return List[int] where List[TransactionId] is expected.\n    # Mypy, Pyright, Zuban should flag this as an error. Pyre might not.\n    return get_raw_ids()\n\ndef process_ids(ids: List[TransactionId]) -> None:\n    \"\"\"Processes a list of TransactionIds.\"\"\"\n    print(f\"Processing IDs of type {type(ids[0]) if ids else 'empty'}: {ids}\")\n    # ids.append(200) # This should be a type error in all, as 200 is int, not TransactionId\n    # ids.append(TransactionId(200)) # This is fine\n\nif __name__ == \"__main__\":\n    print(\"--- Demonstrating correct usage ---\")\n    strict_transactions: List[TransactionId] = [TransactionId(1), TransactionId(2)]\n    process_ids(strict_transactions)\n\n    print(\"\\n--- Demonstrating divergence ---\")\n    # Calling the problematic function. The type checker divergence occurs here\n    # during the return statement's assignment check.\n    try_transactions = fetch_transaction_ids()\n    reveal_type(try_transactions) # If no error, this reveals List[TransactionId]\n    process_ids(try_transactions)\n```\n\n---\n\n### Snippet 7: Overload with `Literal` Discrimination\n\n```python\n# id: overload-literal-discrimination-union-fallback\n# EXPECTED:\n#   mypy: `result1` -> `bool`, `result2` -> `int`, `result3` -> `float`, `result4` -> `str`. Correct discrimination.\n#   pyright: `result1` -> `bool`, `result2` -> `int`, `result3` -> `float`, `result4` -> `str`. Correct discrimination.\n#   pyre: `result1` -> `Union[bool, int, float, str]`, `result2` -> `Union[bool, int, float, str]`, etc. Pyre might fail to discriminate fully, especially with multiple Literal overloads and a catch-all fallback, leading to a wider union type.\n#   zuban: `result1` -> `bool`, `result2` -> `int`, `result3` -> `float`, `result4` -> `str`. Correct discrimination.\n# REASON: Type checkers differ in their ability to perform precise overload resolution, particularly when `Literal` types are used for discrimination alongside a general \"catch-all\" overload. Some checkers correctly identify the most specific matching overload for a given literal argument, while others might fall back to a wider union of all possible return types from all overloads, even for arguments that clearly match a specific `Literal`.\nfrom typing import overload, Literal, Union\n\n@overload\ndef parse_config_value(value: Literal[\"true\", \"false\"]) -> bool: ...\n@overload\ndef parse_config_value(value: Literal[\"0\", \"1\", \"2\"]) -> int: ...\n@overload\ndef parse_config_value(value: Literal[\"3.14\", \"2.71\"]) -> float: ...\n@overload\ndef parse_config_value(value: str) -> str: ... # Catch-all for any other string\n\ndef parse_config_value(value: str) -> Union[bool, int, float, str]:\n    \"\"\"Parses a configuration string into a more specific type.\"\"\"\n    if value in (\"true\", \"false\"):\n        return value == \"true\"\n    if value in (\"0\", \"1\", \"2\"):\n        return int(value)\n    if value in (\"3.14\", \"2.71\"):\n        return float(value)\n    return value\n\nif __name__ == \"__main__\":\n    # Test literal string arguments\n    result1 = parse_config_value(\"true\")\n    reveal_type(result1) # EXPECTED: bool (pyre might show Union[bool, int, float, str])\n\n    result2 = parse_config_value(\"1\")\n    reveal_type(result2) # EXPECTED: int (pyre might show Union[bool, int, float, str])\n\n    result3 = parse_config_value(\"3.14\")\n    reveal_type(result3) # EXPECTED: float (pyre might show Union[bool, int, float, str])\n\n    result4 = parse_config_value(\"any_other_string\")\n    reveal_type(result4) # EXPECTED: str (all should agree, as it hits the fallback)\n\n    # Test with a variable of type str (should always resolve to the widest union)\n    my_str_var: str = \"dynamic_value\"\n    result_var = parse_config_value(my_str_var)\n    reveal_type(result_var) # EXPECTED: Union[bool, int, float, str] (all should agree here)\n```\n\n---\n\n### Snippet 8: `Final` Attribute Override with Property\n\n```python\n# id: final-attribute-override-property\n# EXPECTED:\n#   mypy: Error on `DetailedAppConfig.MODE`. Mypy flags attempts to redefine `Final` attributes, even with properties.\n#   pyright: Error on `DetailedAppConfig.MODE`. Pyright is also strict about `Final` attributes and redefinition.\n#   pyre: No error. Pyre has been observed to be less strict about `Final` attributes in class hierarchies, sometimes allowing a property to override a `Final` field. It might treat the property as a different \"kind\" of member.\n#   zuban: Error. Strict `Final` enforcement, considering redefinition via a property as a violation.\n# REASON: Type checkers differ in their interpretation of `Final` attributes when a derived class attempts to override them with a property (or a method) of the same name. Some checkers strictly enforce that a `Final` member cannot be redefined in any form, while others might consider a property to be a distinct kind of member that doesn't constitute a direct override in the same way a field would, thereby allowing it.\nfrom typing import Final\n\nclass BaseAppConfig:\n    \"\"\"Base class with a final configuration mode.\"\"\"\n    MODE: Final[str] = \"PRODUCTION\" # This is a final class attribute\n\nclass DetailedAppConfig(BaseAppConfig):\n    \"\"\"Derived class attempting to override MODE with a property.\"\"\"\n    @property\n    def MODE(self) -> str: # This property has the same name as the Final attribute in BaseAppConfig\n        print(\"Accessing DetailedAppConfig.MODE property\")\n        return \"DEVELOPMENT\"\n\nclass AnotherAppConfig(BaseAppConfig):\n    \"\"\"Derived class attempting to override MODE with a regular attribute.\"\"\"\n    MODE: str = \"STAGING\" # This should be an error for all strict checkers.\n\ndef get_app_mode(config: BaseAppConfig) -> str:\n    return config.MODE\n\nif __name__ == \"__main__\":\n    print(\"--- Testing DetailedAppConfig ---\")\n    detailed_config = DetailedAppConfig()\n    # The divergence occurs during the definition of DetailedAppConfig.MODE\n    # or when assigning an instance of it to a BaseAppConfig variable.\n    print(f\"Detailed config mode (direct): {detailed_config.MODE}\")\n    reveal_type(detailed_config.MODE) # Should be str\n\n    # Assigning to a base class type hint.\n    # If the property override is allowed, this might change behavior.\n    base_view_config: BaseAppConfig = detailed_config\n    print(f\"Detailed config mode (via BaseAppConfig): {base_view_config.MODE}\")\n    reveal_type(base_view_config.MODE) # Still str, but definition caused the issue.\n\n    print(\"\\n--- Testing AnotherAppConfig (expected error for all) ---\")\n    # This assignment should cause a type error for all strict checkers,\n    # as it's a direct redefinition of a Final class attribute.\n    # another_config: BaseAppConfig = AnotherAppConfig()\n    # print(f\"Another config mode: {another_config.MODE}\")\n```\n\n---\n\n### Snippet 9: Complex Generic Bounds (TypeVar Nesting)\n\n```python\n# id: complex-generic-bounds-typevar-nesting\n# EXPECTED:\n#   mypy: No error. Mypy's recent versions have improved significantly in handling complex generic bounds.\n#   pyright: No error. Pyright is generally robust in generic type inference and bound checking.\n#   pyre: Error on `process_data_source(db_source)` or `reveal_type(first_data)`. Pyre might struggle to correctly resolve `T_Source`'s bound (`DataSource[T_Data]`), leading to an error when an instance of a concrete generic class is passed.\n#   zuban: No error. Aims for advanced generic reasoning and correctness.\n# REASON: Type checkers vary in their ability to handle complex generic bounds, especially when a `TypeVar`'s bound is itself a generic type parameterized by another `TypeVar`. This can lead to difficulties in determining subtype compatibility when an instance of a concrete generic class is passed to a function expecting such a deeply-bound `TypeVar`.\nfrom typing import TypeVar, Generic, List, Protocol\n\nclass BaseData:\n    \"\"\"Base for all data objects.\"\"\"\n    id: int = 0\n\nclass UserData(BaseData):\n    \"\"\"Specific user data.\"\"\"\n    name: str = \"anon\"\n\nclass ProductData(BaseData):\n    \"\"\"Specific product data.\"\"\"\n    sku: str = \"N/A\"\n\n# T_Data is bound to a base data type\nT_Data = TypeVar('T_Data', bound=BaseData)\n\n# DataSource is a generic class that manages a list of T_Data\nclass DataSource(Generic[T_Data]):\n    def __init__(self, data_items: List[T_Data]) -> None:\n        self.data_items = data_items\n\n    def get_first(self) -> T_Data:\n        return self.data_items[0]\n\n# T_Source is a TypeVar whose bound is a DataSource that manages *any* T_Data\n# This is the complex part: T_Source bound to a generic type that itself uses a TypeVar\nT_Source = TypeVar('T_Source', bound=DataSource[T_Data])\n\ndef process_data_source(source: T_Source) -> None:\n    \"\"\"Function that processes a data source bound by T_Source.\"\"\"\n    first_data = source.get_first()\n    print(f\"Processing source. First item ID: {first_data.id}\")\n    reveal_type(source)      # Expected: DataSource[UserData] or DataSource[ProductData]\n    reveal_type(first_data)  # Expected: UserData or ProductData\n\nif __name__ == \"__main__\":\n    user_data_list: List[UserData] = [UserData(id=1, name=\"Alice\"), UserData(id=2, name=\"Bob\")]\n    user_source = DataSource(user_data_list)\n\n    print(\"--- Processing UserData Source ---\")\n    # This is the critical call for divergence:\n    # Does `user_source` (DataSource[UserData]) fit the bound `T_Source` (bound=DataSource[T_Data])?\n    process_data_source(user_source) # Pyre might error here.\n\n    product_data_list: List[ProductData] = [ProductData(id=10, sku=\"A123\"), ProductData(id=11, sku=\"B456\")]\n    product_source = DataSource(product_data_list)\n\n    print(\"\\n--- Processing ProductData Source ---\")\n    process_data_source(product_source) # Pyre might error here too.\n```\n\n---\n\n### Snippet 10: Protocol for `__call__` (Callable) with Positional vs. Keyword Arguments\n\n```python\n# id: protocol-callable-keyword-only-vs-positional-or-keyword\n# EXPECTED:\n#   mypy: No error on `validator: DataValidator = positional_validator`. Mypy typically allows positional-or-keyword arguments to satisfy keyword-only requirements if names and types match.\n#   pyright: Error on `validator: DataValidator = positional_validator` and `validator: DataValidator = positional_no_default_validator`. Pyright is stricter about `keyword-only` in protocols, enforcing the `*` separator.\n#   pyre: Error on `validator: DataValidator = positional_validator` and `validator: DataValidator = positional_no_default_validator`. Pyre often aligns with stricter interpretations of callable compatibility.\n#   zuban: Error on `validator: DataValidator = positional_validator` and `validator: DataValidator = positional_no_default_validator`. Follows strict keyword-only enforcement.\n# REASON: Type checkers differ in their interpretation of compatibility between a `Protocol` requiring keyword-only arguments for its `__call__` method and a concrete function that defines those arguments as positional-or-keyword. Some consider this compatible as long as the types and names match, while others strictly enforce the keyword-only nature introduced by the `*` separator in the protocol's signature.\nfrom typing import Protocol, Callable\n\nclass DataValidator(Protocol):\n    \"\"\"Protocol for a callable that validates data with keyword-only arguments.\"\"\"\n    def __call__(self, *, data: dict[str, str], strict: bool = True) -> bool: ...\n\ndef keyword_only_validator(*, data: dict[str, str], strict: bool = True) -> bool:\n    \"\"\"Function with explicit keyword-only arguments, matching the protocol.\"\"\"\n    print(f\"  Keyword-only validator called (strict={strict})\")\n    return \"value\" in data and (strict or \"optional\" not in data)\n\ndef positional_validator(data: dict[str, str], strict: bool = True) -> bool:\n    \"\"\"Function with positional-or-keyword arguments.\"\"\"\n    print(f\"  Positional-or-keyword validator called (strict={strict})\")\n    return \"value\" in data and (strict or \"optional\" not in data)\n\ndef positional_no_default_validator(data: dict[str, str], strict: bool) -> bool:\n    \"\"\"Function with positional-or-keyword arguments and no default for strict.\"\"\"\n    print(f\"  Positional (no default) validator called (strict={strict})\")\n    return \"value\" in data and (strict or \"optional\" not in data)\n\ndef use_validator(validator_func: DataValidator, payload: dict[str, str]) -> None:\n    result_default = validator_func(data=payload)\n    print(f\"    Validation result (default strict): {result_default}\")\n    result_strict = validator_func(data=payload, strict=False)\n    print(f\"    Validation result (strict=False): {result_strict}\")\n\nif __name__ == \"__main__\":\n    sample_data = {\"key\": \"value\", \"optional\": \"true\"}\n\n    print(\"--- Testing keyword_only_validator (all should pass) ---\")\n    validator_kw: DataValidator = keyword_only_validator\n    use_validator(validator_kw, sample_data)\n    reveal_type(validator_kw)\n\n    print(\"\\n--- Testing positional_validator (mypy passes, others diverge) ---\")\n    # This is the main divergence point:\n    # Mypy is expected to allow this, Pyright/Pyre/Zuban to flag an error.\n    validator_pos: DataValidator = positional_validator\n    use_validator(validator_pos, sample_data)\n    reveal_type(validator_pos)\n\n    print(\"\\n--- Testing positional_no_default_validator (most/all should fail) ---\")\n    # This should fail for Pyright/Pyre/Zuban due to keyword-only + missing default.\n    # Mypy *might* still allow it if it only checks callability and type, but the missing default\n    # for `strict` when the protocol expects one makes it less likely even for mypy.\n    validator_pos_no_default: DataValidator = positional_no_default_validator\n    use_validator(validator_pos_no_default, sample_data)\n    reveal_type(validator_pos_no_default)\n```\n\n---\n\nI performed one comprehensive round of generating and refining these 10 examples, simulating the type checker behavior based on known divergences and typical strictness levels for each tool.",
    "examples": [
        {
            "id": "protocol-default-arg-absence",
            "metadata": "# EXPECTED:\n#   mypy: No error. Mypy considers `path: str` compatible with `path: str = ...` in an implementation for a protocol.\n#   pyright: No error. Pyright typically allows this as long as the implementation's argument types and positions are compatible with all possible calls allowed by the protocol.\n#   pyre: Error. Pyre is often stricter, flagging an incompatibility because `FileConfigLoader.load` lacks a default argument, while `ConfigLoader.load` specifies one, which would make a call like `loader.load()` ambiguous for `FileConfigLoader`.\n#   zuban: No error. Focuses on call signature compatibility (i.e., whether the implementation can satisfy all calls).\n# REASON: Type checkers differ on whether the presence or absence of a default argument in an implementation, when the protocol specifies one, constitutes a signature mismatch. Some prioritize callable signature compatibility (parameters, types), allowing an implementation to omit a default if it can still satisfy calls where the argument is either provided or expected to be optional. Others view the default argument as part of the callable's contract that must be present.",
            "code": "from typing import Protocol\n\nclass ConfigLoader(Protocol):\n    \"\"\"Protocol requiring a load method with an optional path.\"\"\"\n    def load(self, path: str = \"default.ini\") -> dict[str, str]: ...\n\nclass FileConfigLoader:\n    \"\"\"Implementation that omits the default argument.\"\"\"\n    def load(self, path: str) -> dict[str, str]: # Protocol specifies default, implementation does not\n        print(f\"FileConfigLoader: Loading from {path}\")\n        return {\"file_key\": \"file_value\"}\n\nclass DefaultConfigLoader:\n    \"\"\"Implementation with a different default argument.\"\"\"\n    def load(self, path: str = \"another.cfg\") -> dict[str, str]: # Protocol specifies default, implementation has different default\n        print(f\"DefaultConfigLoader: Loading from {path} with different default\")\n        return {\"default_key\": \"default_value\"}\n\ndef process_config(loader: ConfigLoader) -> None:\n    # This call relies on the protocol's default argument\n    # Pyre is expected to flag an error here if `loader` is `FileConfigLoader` because `FileConfigLoader.load` has no default.\n    config = loader.load()\n    print(f\"    Processed config (default): {config}\")\n\n    # This call explicitly provides an argument\n    config = loader.load(\"custom.json\")\n    print(f\"    Processed config (custom): {config}\")\n\nif __name__ == \"__main__\":\n    print(\"--- Testing FileConfigLoader ---\")\n    file_loader: ConfigLoader = FileConfigLoader() # Potential divergence point\n    process_config(file_loader)\n    \n    print(\"\\n--- Testing DefaultConfigLoader ---\")\n    default_loader: ConfigLoader = DefaultConfigLoader() # All should typically allow\n    process_config(default_loader)\n\n---\n\n### Snippet 2: TypeGuard with Generic Narrowing",
            "full_content": "# id: protocol-default-arg-absence\n# EXPECTED:\n#   mypy: No error. Mypy considers `path: str` compatible with `path: str = ...` in an implementation for a protocol.\n#   pyright: No error. Pyright typically allows this as long as the implementation's argument types and positions are compatible with all possible calls allowed by the protocol.\n#   pyre: Error. Pyre is often stricter, flagging an incompatibility because `FileConfigLoader.load` lacks a default argument, while `ConfigLoader.load` specifies one, which would make a call like `loader.load()` ambiguous for `FileConfigLoader`.\n#   zuban: No error. Focuses on call signature compatibility (i.e., whether the implementation can satisfy all calls).\n# REASON: Type checkers differ on whether the presence or absence of a default argument in an implementation, when the protocol specifies one, constitutes a signature mismatch. Some prioritize callable signature compatibility (parameters, types), allowing an implementation to omit a default if it can still satisfy calls where the argument is either provided or expected to be optional. Others view the default argument as part of the callable's contract that must be present.\n\nfrom typing import Protocol\n\nclass ConfigLoader(Protocol):\n    \"\"\"Protocol requiring a load method with an optional path.\"\"\"\n    def load(self, path: str = \"default.ini\") -> dict[str, str]: ...\n\nclass FileConfigLoader:\n    \"\"\"Implementation that omits the default argument.\"\"\"\n    def load(self, path: str) -> dict[str, str]: # Protocol specifies default, implementation does not\n        print(f\"FileConfigLoader: Loading from {path}\")\n        return {\"file_key\": \"file_value\"}\n\nclass DefaultConfigLoader:\n    \"\"\"Implementation with a different default argument.\"\"\"\n    def load(self, path: str = \"another.cfg\") -> dict[str, str]: # Protocol specifies default, implementation has different default\n        print(f\"DefaultConfigLoader: Loading from {path} with different default\")\n        return {\"default_key\": \"default_value\"}\n\ndef process_config(loader: ConfigLoader) -> None:\n    # This call relies on the protocol's default argument\n    # Pyre is expected to flag an error here if `loader` is `FileConfigLoader` because `FileConfigLoader.load` has no default.\n    config = loader.load()\n    print(f\"    Processed config (default): {config}\")\n\n    # This call explicitly provides an argument\n    config = loader.load(\"custom.json\")\n    print(f\"    Processed config (custom): {config}\")\n\nif __name__ == \"__main__\":\n    print(\"--- Testing FileConfigLoader ---\")\n    file_loader: ConfigLoader = FileConfigLoader() # Potential divergence point\n    process_config(file_loader)\n    \n    print(\"\\n--- Testing DefaultConfigLoader ---\")\n    default_loader: ConfigLoader = DefaultConfigLoader() # All should typically allow\n    process_config(default_loader)\n\n---\n\n### Snippet 2: TypeGuard with Generic Narrowing"
        },
        {
            "id": "typeguard-generic-narrowing",
            "metadata": "# EXPECTED:\n#   mypy: No error. Mypy successfully narrows `data` to `list[int]` within the `if` block. `reveal_type(data)` -> `list[int]`.\n#   pyright: No error. Pyright is very strong with TypeGuard and generic narrowing. `reveal_type(data)` -> `list[int]`.\n#   pyre: Error on `data.append(\"string\")`. Pyre might not fully narrow `data` to `list[int]`, potentially leaving it as `list[Union[int, str]]` or a less specific type, thus allowing a `str` to be appended even after the guard. Or it might be conservative about the narrowing itself.\n#   zuban: No error. Aims for precise TypeGuard application.\n# REASON: Type checkers differ in their ability to perform precise generic narrowing, especially when a TypeGuard makes assertions about the elements of a generic container. Some might not fully propagate the narrowing to the container's type parameter, leading to issues with subsequent operations that should be restricted by the narrowed type.",
            "code": "from typing import TypeGuard, TypeVar, List, Union\n\nT = TypeVar('T')\n\ndef is_all_type(val: List[object], target_type: type[T]) -> TypeGuard[List[T]]:\n    \"\"\"TypeGuard that checks if all elements in the list are of a specific type.\"\"\"\n    return all(isinstance(x, target_type) for x in val)\n\ndef process_items(data: List[Union[int, str, bool]]) -> None:\n    print(f\"Initial data: {data}\")\n\n    if is_all_type(data, int):\n        print(\"  Inside 'is_all_type(int)' block.\")\n        # Inside this block, 'data' should be narrowed to 'list[int]'\n        data.append(100) # This should be allowed by all\n        reveal_type(data) # Expected: list[int]\n\n        # Pyre might flag this if it didn't fully narrow, incorrectly thinking 'str' is still possible.\n        # Or, conversely, if it fails to narrow, it might *not* flag data.append(\"string\") below.\n        # The divergence is often in what *is* allowed/disallowed due to narrowing.\n        print(f\"  Data after appending int: {data}\")\n\n    print(\"  Outside 'is_all_type(int)' block.\")\n    # Outside the if block, data is still List[Union[int, str, bool]]\n    # This should be allowed by all.\n    data.append(\"new string\")\n    print(f\"  Data after appending str: {data}\")\n\n    # This should always be an error, but if Pyre *fails* to narrow,\n    # it might allow appending `str` inside the `if` block, which would be a divergence.\n    # The snippet demonstrates a valid operation (append int) when narrowed,\n    # expecting Pyre to potentially fail on the type assertion itself,\n    # or fail to narrow so that a later invalid append is not caught.\n    # Let's add an *explicitly wrong* append for narrowed type.\n    if is_all_type(data, str):\n        data.append(123) # This should be an error for all, but TypeGuard itself is the focus.\n\nif __name__ == \"__main__\":\n    my_mixed_list: List[Union[int, str, bool]] = [1, 2, \"three\", True, 4]\n    process_items(my_mixed_list)\n\n    print(\"\\n--- Processing all integers list ---\")\n    my_int_list: List[Union[int, str, bool]] = [10, 20, 30]\n    process_items(my_int_list)\n\n---\n\n### Snippet 3: TypedDict with `Required`/`NotRequired` and `total=False` mixed semantics",
            "full_content": "# id: typeguard-generic-narrowing\n# EXPECTED:\n#   mypy: No error. Mypy successfully narrows `data` to `list[int]` within the `if` block. `reveal_type(data)` -> `list[int]`.\n#   pyright: No error. Pyright is very strong with TypeGuard and generic narrowing. `reveal_type(data)` -> `list[int]`.\n#   pyre: Error on `data.append(\"string\")`. Pyre might not fully narrow `data` to `list[int]`, potentially leaving it as `list[Union[int, str]]` or a less specific type, thus allowing a `str` to be appended even after the guard. Or it might be conservative about the narrowing itself.\n#   zuban: No error. Aims for precise TypeGuard application.\n# REASON: Type checkers differ in their ability to perform precise generic narrowing, especially when a TypeGuard makes assertions about the elements of a generic container. Some might not fully propagate the narrowing to the container's type parameter, leading to issues with subsequent operations that should be restricted by the narrowed type.\n\nfrom typing import TypeGuard, TypeVar, List, Union\n\nT = TypeVar('T')\n\ndef is_all_type(val: List[object], target_type: type[T]) -> TypeGuard[List[T]]:\n    \"\"\"TypeGuard that checks if all elements in the list are of a specific type.\"\"\"\n    return all(isinstance(x, target_type) for x in val)\n\ndef process_items(data: List[Union[int, str, bool]]) -> None:\n    print(f\"Initial data: {data}\")\n\n    if is_all_type(data, int):\n        print(\"  Inside 'is_all_type(int)' block.\")\n        # Inside this block, 'data' should be narrowed to 'list[int]'\n        data.append(100) # This should be allowed by all\n        reveal_type(data) # Expected: list[int]\n\n        # Pyre might flag this if it didn't fully narrow, incorrectly thinking 'str' is still possible.\n        # Or, conversely, if it fails to narrow, it might *not* flag data.append(\"string\") below.\n        # The divergence is often in what *is* allowed/disallowed due to narrowing.\n        print(f\"  Data after appending int: {data}\")\n\n    print(\"  Outside 'is_all_type(int)' block.\")\n    # Outside the if block, data is still List[Union[int, str, bool]]\n    # This should be allowed by all.\n    data.append(\"new string\")\n    print(f\"  Data after appending str: {data}\")\n\n    # This should always be an error, but if Pyre *fails* to narrow,\n    # it might allow appending `str` inside the `if` block, which would be a divergence.\n    # The snippet demonstrates a valid operation (append int) when narrowed,\n    # expecting Pyre to potentially fail on the type assertion itself,\n    # or fail to narrow so that a later invalid append is not caught.\n    # Let's add an *explicitly wrong* append for narrowed type.\n    if is_all_type(data, str):\n        data.append(123) # This should be an error for all, but TypeGuard itself is the focus.\n\nif __name__ == \"__main__\":\n    my_mixed_list: List[Union[int, str, bool]] = [1, 2, \"three\", True, 4]\n    process_items(my_mixed_list)\n\n    print(\"\\n--- Processing all integers list ---\")\n    my_int_list: List[Union[int, str, bool]] = [10, 20, 30]\n    process_items(my_int_list)\n\n---\n\n### Snippet 3: TypedDict with `Required`/`NotRequired` and `total=False` mixed semantics"
        },
        {
            "id": "typeddict-total-mixed-required-notrequired",
            "metadata": "# EXPECTED:\n#   mypy: No error. All `reveal_type`s should be `Union[Type, None]` for `.get()`. `total=False` propagates.\n#   pyright: No error. All `reveal_type`s should be `Type | None` for `.get()`. `Required` overrides `total=False` for specific keys.\n#   pyre: Subtle error/difference. Pyre might treat 'optional_param' as strictly missing if `total=False` isn't fully propagated for inherited keys, leading to an error on `td.get('optional_param')` or incorrect `reveal_type`. Or it might raise an error during assignment if it misinterprets the optionality.\n#   zuban: No error. Aims for precise TypedDict semantics.\n# REASON: The interaction between `total=False` on a base `TypedDict` and explicit `Required`/`NotRequired` in a derived one can lead to different interpretations of key optionality. This divergence is often seen in how `.get()` is typed for keys whose optionality is implicitly inherited versus explicitly defined, or in how `total=False` inheritance affects new keys.",
            "code": "from typing import TypedDict, Union\nfrom typing_extensions import Required, NotRequired\n\nclass BaseOpts(TypedDict, total=False):\n    \"\"\"Base TypedDict where all keys are optional by default.\"\"\"\n    config_id: int\n    log_file: str\n\nclass AdvancedOpts(BaseOpts):\n    \"\"\"Derived TypedDict, mixing explicit optionality and inheriting total=False.\"\"\"\n    # new_param is implicitly NotRequired due to inheriting total=False\n    new_param: float\n    \n    # Required keys explicitly override total=False for themselves\n    api_key: Required[str]\n    \n    # Explicitly NotRequired (redundant given total=False, but allowed)\n    debug_mode: NotRequired[bool]\n\ndef process_options(opts: AdvancedOpts) -> None:\n    # 'api_key' is Required, but .get() always returns Optional.\n    api_k = opts.get('api_key')\n    reveal_type(api_k) # EXPECTED: str | None (all should agree)\n    if api_k:\n        print(f\"API Key: {api_k}\")\n\n    # 'config_id' is implicitly NotRequired from BaseOpts(total=False)\n    cfg_id = opts.get('config_id')\n    reveal_type(cfg_id) # EXPECTED: int | None (pyre might differ here or on the call)\n    if cfg_id is not None:\n        print(f\"Config ID: {cfg_id}\")\n\n    # 'new_param' is implicitly NotRequired as it's a new key in a total=False derived TypedDict.\n    new_p = opts.get('new_param')\n    reveal_type(new_p) # EXPECTED: float | None (pyre might differ here)\n    if new_p is not None:\n        print(f\"New Param: {new_p}\")\n\n    # 'debug_mode' is explicitly NotRequired.\n    dbg_m = opts.get('debug_mode')\n    reveal_type(dbg_m) # EXPECTED: bool | None (all should agree)\n    if dbg_m is not None:\n        print(f\"Debug Mode: {dbg_m}\")\n\nif __name__ == \"__main__\":\n    # Valid config: includes 'api_key' (Required), others are optional\n    config1: AdvancedOpts = {'api_key': 'abc-123', 'config_id': 101, 'new_param': 3.14}\n    print(\"--- Processing config1 ---\")\n    process_options(config1)\n\n    # Config with only required key\n    config2: AdvancedOpts = {'api_key': 'xyz-456'}\n    print(\"\\n--- Processing config2 ---\")\n    process_options(config2)\n\n    # This assignment would fail for all checkers if 'api_key' is missing.\n    # config3: AdvancedOpts = {'config_id': 202}\n\n---\n\n### Snippet 4: `ParamSpec` with Decorators on Class Methods",
            "full_content": "# id: typeddict-total-mixed-required-notrequired\n# EXPECTED:\n#   mypy: No error. All `reveal_type`s should be `Union[Type, None]` for `.get()`. `total=False` propagates.\n#   pyright: No error. All `reveal_type`s should be `Type | None` for `.get()`. `Required` overrides `total=False` for specific keys.\n#   pyre: Subtle error/difference. Pyre might treat 'optional_param' as strictly missing if `total=False` isn't fully propagated for inherited keys, leading to an error on `td.get('optional_param')` or incorrect `reveal_type`. Or it might raise an error during assignment if it misinterprets the optionality.\n#   zuban: No error. Aims for precise TypedDict semantics.\n# REASON: The interaction between `total=False` on a base `TypedDict` and explicit `Required`/`NotRequired` in a derived one can lead to different interpretations of key optionality. This divergence is often seen in how `.get()` is typed for keys whose optionality is implicitly inherited versus explicitly defined, or in how `total=False` inheritance affects new keys.\n\nfrom typing import TypedDict, Union\nfrom typing_extensions import Required, NotRequired\n\nclass BaseOpts(TypedDict, total=False):\n    \"\"\"Base TypedDict where all keys are optional by default.\"\"\"\n    config_id: int\n    log_file: str\n\nclass AdvancedOpts(BaseOpts):\n    \"\"\"Derived TypedDict, mixing explicit optionality and inheriting total=False.\"\"\"\n    # new_param is implicitly NotRequired due to inheriting total=False\n    new_param: float\n    \n    # Required keys explicitly override total=False for themselves\n    api_key: Required[str]\n    \n    # Explicitly NotRequired (redundant given total=False, but allowed)\n    debug_mode: NotRequired[bool]\n\ndef process_options(opts: AdvancedOpts) -> None:\n    # 'api_key' is Required, but .get() always returns Optional.\n    api_k = opts.get('api_key')\n    reveal_type(api_k) # EXPECTED: str | None (all should agree)\n    if api_k:\n        print(f\"API Key: {api_k}\")\n\n    # 'config_id' is implicitly NotRequired from BaseOpts(total=False)\n    cfg_id = opts.get('config_id')\n    reveal_type(cfg_id) # EXPECTED: int | None (pyre might differ here or on the call)\n    if cfg_id is not None:\n        print(f\"Config ID: {cfg_id}\")\n\n    # 'new_param' is implicitly NotRequired as it's a new key in a total=False derived TypedDict.\n    new_p = opts.get('new_param')\n    reveal_type(new_p) # EXPECTED: float | None (pyre might differ here)\n    if new_p is not None:\n        print(f\"New Param: {new_p}\")\n\n    # 'debug_mode' is explicitly NotRequired.\n    dbg_m = opts.get('debug_mode')\n    reveal_type(dbg_m) # EXPECTED: bool | None (all should agree)\n    if dbg_m is not None:\n        print(f\"Debug Mode: {dbg_m}\")\n\nif __name__ == \"__main__\":\n    # Valid config: includes 'api_key' (Required), others are optional\n    config1: AdvancedOpts = {'api_key': 'abc-123', 'config_id': 101, 'new_param': 3.14}\n    print(\"--- Processing config1 ---\")\n    process_options(config1)\n\n    # Config with only required key\n    config2: AdvancedOpts = {'api_key': 'xyz-456'}\n    print(\"\\n--- Processing config2 ---\")\n    process_options(config2)\n\n    # This assignment would fail for all checkers if 'api_key' is missing.\n    # config3: AdvancedOpts = {'config_id': 202}\n\n---\n\n### Snippet 4: `ParamSpec` with Decorators on Class Methods"
        },
        {
            "id": "paramspec-classmethod-decorator-signature",
            "metadata": "# EXPECTED:\n#   mypy: Error. Mypy has historically struggled with ParamSpec on classmethod/staticmethod, often losing the `cls` argument or misinterpreting the signature, leading to an error when calling `Service.create_named_resource`.\n#   pyright: No error. Pyright is generally very robust with ParamSpec, including on class methods, correctly inferring the `cls` argument's behavior.\n#   pyre: Error. Similar to mypy's historical issues, Pyre might misinterpret the signature after decoration.\n#   zuban: No error. Aims for high compatibility with modern Python typing features.\n# REASON: Type checkers differ in their ability to correctly infer and preserve the signature of class methods when they are decorated by a `ParamSpec`-aware decorator. The implicit `cls` (or `self`) argument can be mishandled, leading to incorrect call signatures or type errors when the method is invoked, as `ParamSpec` needs to correctly capture or omit it.",
            "code": "from typing import TypeVar, Callable, ParamSpec, Type\nfrom typing_extensions import Concatenate # Explicitly show usage though not strictly needed for this example\n\nP = ParamSpec('P')\nR = TypeVar('R')\nClsT = TypeVar('ClsT') # Type variable for the class type in classmethods\n\ndef log_and_return(func: Callable[P, R]) -> Callable[P, R]:\n    \"\"\"A simple decorator that logs and returns the result.\"\"\"\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:\n        print(f\"LOG: Calling '{func.__name__}' with args={args}, kwargs={kwargs}\")\n        result = func(*args, **kwargs)\n        print(f\"LOG: '{func.__name__}' returned: {result}\")\n        return result\n    return wrapper\n\nclass Resource:\n    _registry: dict[str, 'Resource'] = {}\n\n    def __init__(self, name: str, value: int) -> None:\n        self.name = name\n        self.value = value\n        self.__class__._registry[name] = self\n\n    @log_and_return\n    @classmethod\n    def create_named_resource(cls: Type[ClsT], name: str, initial_value: int = 0) -> ClsT:\n        \"\"\"A class method to create or retrieve a resource.\"\"\"\n        if name not in cls._registry:\n            print(f\"  Creating new resource: {name}\")\n            return cls(name, initial_value)\n        print(f\"  Retrieving existing resource: {name}\")\n        return cls._registry[name] # type: ignore [return-value] # This ignore is for runtime, type checkers handle it\n\nif __name__ == \"__main__\":\n    print(\"--- Calling create_named_resource ---\")\n    # This call should be type-checked based on `create_named_resource`'s\n    # original signature, correctly handling `cls` and `name: str, initial_value: int`.\n    resource1 = Resource.create_named_resource(\"Alpha\", 10)\n    print(f\"Resource 1: {resource1.name}, {resource1.value}\")\n    reveal_type(resource1) # EXPECTED: Resource\n\n    resource2 = Resource.create_named_resource(\"Beta\") # Uses default initial_value\n    print(f\"Resource 2: {resource2.name}, {resource2.value}\")\n\n    # The actual divergence will be seen here:\n    # Mypy/Pyre might reveal a signature that doesn't include 'initial_value' or misattributes 'name'\n    reveal_type(Resource.create_named_resource) # Check the inferred signature\n\n---\n\n### Snippet 5: `Self` in Generics with Abstract Methods",
            "full_content": "# id: paramspec-classmethod-decorator-signature\n# EXPECTED:\n#   mypy: Error. Mypy has historically struggled with ParamSpec on classmethod/staticmethod, often losing the `cls` argument or misinterpreting the signature, leading to an error when calling `Service.create_named_resource`.\n#   pyright: No error. Pyright is generally very robust with ParamSpec, including on class methods, correctly inferring the `cls` argument's behavior.\n#   pyre: Error. Similar to mypy's historical issues, Pyre might misinterpret the signature after decoration.\n#   zuban: No error. Aims for high compatibility with modern Python typing features.\n# REASON: Type checkers differ in their ability to correctly infer and preserve the signature of class methods when they are decorated by a `ParamSpec`-aware decorator. The implicit `cls` (or `self`) argument can be mishandled, leading to incorrect call signatures or type errors when the method is invoked, as `ParamSpec` needs to correctly capture or omit it.\n\nfrom typing import TypeVar, Callable, ParamSpec, Type\nfrom typing_extensions import Concatenate # Explicitly show usage though not strictly needed for this example\n\nP = ParamSpec('P')\nR = TypeVar('R')\nClsT = TypeVar('ClsT') # Type variable for the class type in classmethods\n\ndef log_and_return(func: Callable[P, R]) -> Callable[P, R]:\n    \"\"\"A simple decorator that logs and returns the result.\"\"\"\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:\n        print(f\"LOG: Calling '{func.__name__}' with args={args}, kwargs={kwargs}\")\n        result = func(*args, **kwargs)\n        print(f\"LOG: '{func.__name__}' returned: {result}\")\n        return result\n    return wrapper\n\nclass Resource:\n    _registry: dict[str, 'Resource'] = {}\n\n    def __init__(self, name: str, value: int) -> None:\n        self.name = name\n        self.value = value\n        self.__class__._registry[name] = self\n\n    @log_and_return\n    @classmethod\n    def create_named_resource(cls: Type[ClsT], name: str, initial_value: int = 0) -> ClsT:\n        \"\"\"A class method to create or retrieve a resource.\"\"\"\n        if name not in cls._registry:\n            print(f\"  Creating new resource: {name}\")\n            return cls(name, initial_value)\n        print(f\"  Retrieving existing resource: {name}\")\n        return cls._registry[name] # type: ignore [return-value] # This ignore is for runtime, type checkers handle it\n\nif __name__ == \"__main__\":\n    print(\"--- Calling create_named_resource ---\")\n    # This call should be type-checked based on `create_named_resource`'s\n    # original signature, correctly handling `cls` and `name: str, initial_value: int`.\n    resource1 = Resource.create_named_resource(\"Alpha\", 10)\n    print(f\"Resource 1: {resource1.name}, {resource1.value}\")\n    reveal_type(resource1) # EXPECTED: Resource\n\n    resource2 = Resource.create_named_resource(\"Beta\") # Uses default initial_value\n    print(f\"Resource 2: {resource2.name}, {resource2.value}\")\n\n    # The actual divergence will be seen here:\n    # Mypy/Pyre might reveal a signature that doesn't include 'initial_value' or misattributes 'name'\n    reveal_type(Resource.create_named_resource) # Check the inferred signature\n\n---\n\n### Snippet 5: `Self` in Generics with Abstract Methods"
        },
        {
            "id": "self-in-generic-abstract-method",
            "metadata": "# EXPECTED:\n#   mypy: Error on `cloned_processor: StringProcessor = processor_instance.duplicate()`. Mypy might resolve `Self` in the generic context to `BaseProcessor[T]` instead of the specific concrete subclass (`StringProcessor`). `reveal_type(cloned_processor_var)` would show `BaseProcessor[str]`.\n#   pyright: No error. Pyright generally handles `Self` correctly even within generic abstract methods, inferring the concrete class type. `reveal_type(cloned_processor_var)` -> `StringProcessor`.\n#   pyre: Error. Pyre often struggles with complex `Self` and generics interactions, similar to mypy.\n#   zuban: No error. Aims for precise type inference for `Self`.\n# REASON: Type checkers differ in how they resolve the `Self` type when it's used as a return type in an abstract method within a generic class. Some might incorrectly resolve `Self` to the generic base class (e.g., `BaseProcessor[str]`), rather than the specific concrete subclass (`StringProcessor`), leading to type mismatches during assignment or further operations.",
            "code": "from abc import ABC, abstractmethod\nfrom typing import Generic, TypeVar, Type\nfrom typing_extensions import Self\n\nT = TypeVar('T')\n\nclass BaseProcessor(ABC, Generic[T]):\n    \"\"\"Abstract generic class for processing data.\"\"\"\n    def __init__(self, data: T) -> None:\n        self._data = data\n\n    @abstractmethod\n    def process(self) -> T:\n        \"\"\"Processes the internal data.\"\"\"\n        ...\n\n    @abstractmethod\n    def duplicate(self) -> Self:\n        \"\"\"Returns a new instance of the concrete processor type, using Self.\"\"\"\n        # The critical point: Self should resolve to the implementing class.\n        ...\n\nclass StringProcessor(BaseProcessor[str]):\n    \"\"\"Concrete processor for string data.\"\"\"\n    def process(self) -> str:\n        return f\"Processed: {self._data.upper()}\"\n\n    def duplicate(self) -> Self:\n        # `Self` should resolve to `StringProcessor` here.\n        # This factory call should return an instance of `StringProcessor`.\n        print(f\"Duplicating {type(self).__name__} with data '{self._data}'\")\n        return type(self)(self._data + \" (copy)\")\n\nclass IntProcessor(BaseProcessor[int]):\n    \"\"\"Concrete processor for integer data.\"\"\"\n    def process(self) -> int:\n        return self._data * 2\n\n    def duplicate(self) -> Self:\n        print(f\"Duplicating {type(self).__name__} with data '{self._data}'\")\n        return type(self)(self._data + 10)\n\n\ndef check_processor(processor: BaseProcessor[str]) -> BaseProcessor[str]:\n    # This call demonstrates that `duplicate` *can* be called via the base type.\n    # The return type should still be compatible with the base generic.\n    return processor.duplicate()\n\nif __name__ == \"__main__\":\n    processor_instance = StringProcessor(\"hello\")\n    print(f\"Original type: {type(processor_instance).__name__}\")\n    \n    # This assignment is the key divergence point.\n    # If Self resolves correctly, this is fine. If not, mypy/pyre will error.\n    cloned_processor: StringProcessor = processor_instance.duplicate()\n    \n    print(f\"Cloned type: {type(cloned_processor).__name__}\")\n    print(f\"Cloned data: {cloned_processor._data}\")\n    reveal_type(cloned_processor) # EXPECTED: StringProcessor\n\n    print(\"\\n--- Testing IntProcessor ---\")\n    int_processor_instance = IntProcessor(10)\n    cloned_int_processor: IntProcessor = int_processor_instance.duplicate()\n    print(f\"Cloned int processor type: {type(cloned_int_processor).__name__}\")\n    reveal_type(cloned_int_processor) # EXPECTED: IntProcessor\n\n    print(\"\\n--- Testing through base type hint ---\")\n    base_processor_ref: BaseProcessor[str] = StringProcessor(\"base_test\")\n    returned_base_processor = check_processor(base_processor_ref)\n    reveal_type(returned_base_processor) # EXPECTED: BaseProcessor[str]\n\n---\n\n### Snippet 6: `NewType` and `List` Contravariance",
            "full_content": "# id: self-in-generic-abstract-method\n# EXPECTED:\n#   mypy: Error on `cloned_processor: StringProcessor = processor_instance.duplicate()`. Mypy might resolve `Self` in the generic context to `BaseProcessor[T]` instead of the specific concrete subclass (`StringProcessor`). `reveal_type(cloned_processor_var)` would show `BaseProcessor[str]`.\n#   pyright: No error. Pyright generally handles `Self` correctly even within generic abstract methods, inferring the concrete class type. `reveal_type(cloned_processor_var)` -> `StringProcessor`.\n#   pyre: Error. Pyre often struggles with complex `Self` and generics interactions, similar to mypy.\n#   zuban: No error. Aims for precise type inference for `Self`.\n# REASON: Type checkers differ in how they resolve the `Self` type when it's used as a return type in an abstract method within a generic class. Some might incorrectly resolve `Self` to the generic base class (e.g., `BaseProcessor[str]`), rather than the specific concrete subclass (`StringProcessor`), leading to type mismatches during assignment or further operations.\n\nfrom abc import ABC, abstractmethod\nfrom typing import Generic, TypeVar, Type\nfrom typing_extensions import Self\n\nT = TypeVar('T')\n\nclass BaseProcessor(ABC, Generic[T]):\n    \"\"\"Abstract generic class for processing data.\"\"\"\n    def __init__(self, data: T) -> None:\n        self._data = data\n\n    @abstractmethod\n    def process(self) -> T:\n        \"\"\"Processes the internal data.\"\"\"\n        ...\n\n    @abstractmethod\n    def duplicate(self) -> Self:\n        \"\"\"Returns a new instance of the concrete processor type, using Self.\"\"\"\n        # The critical point: Self should resolve to the implementing class.\n        ...\n\nclass StringProcessor(BaseProcessor[str]):\n    \"\"\"Concrete processor for string data.\"\"\"\n    def process(self) -> str:\n        return f\"Processed: {self._data.upper()}\"\n\n    def duplicate(self) -> Self:\n        # `Self` should resolve to `StringProcessor` here.\n        # This factory call should return an instance of `StringProcessor`.\n        print(f\"Duplicating {type(self).__name__} with data '{self._data}'\")\n        return type(self)(self._data + \" (copy)\")\n\nclass IntProcessor(BaseProcessor[int]):\n    \"\"\"Concrete processor for integer data.\"\"\"\n    def process(self) -> int:\n        return self._data * 2\n\n    def duplicate(self) -> Self:\n        print(f\"Duplicating {type(self).__name__} with data '{self._data}'\")\n        return type(self)(self._data + 10)\n\n\ndef check_processor(processor: BaseProcessor[str]) -> BaseProcessor[str]:\n    # This call demonstrates that `duplicate` *can* be called via the base type.\n    # The return type should still be compatible with the base generic.\n    return processor.duplicate()\n\nif __name__ == \"__main__\":\n    processor_instance = StringProcessor(\"hello\")\n    print(f\"Original type: {type(processor_instance).__name__}\")\n    \n    # This assignment is the key divergence point.\n    # If Self resolves correctly, this is fine. If not, mypy/pyre will error.\n    cloned_processor: StringProcessor = processor_instance.duplicate()\n    \n    print(f\"Cloned type: {type(cloned_processor).__name__}\")\n    print(f\"Cloned data: {cloned_processor._data}\")\n    reveal_type(cloned_processor) # EXPECTED: StringProcessor\n\n    print(\"\\n--- Testing IntProcessor ---\")\n    int_processor_instance = IntProcessor(10)\n    cloned_int_processor: IntProcessor = int_processor_instance.duplicate()\n    print(f\"Cloned int processor type: {type(cloned_int_processor).__name__}\")\n    reveal_type(cloned_int_processor) # EXPECTED: IntProcessor\n\n    print(\"\\n--- Testing through base type hint ---\")\n    base_processor_ref: BaseProcessor[str] = StringProcessor(\"base_test\")\n    returned_base_processor = check_processor(base_processor_ref)\n    reveal_type(returned_base_processor) # EXPECTED: BaseProcessor[str]\n\n---\n\n### Snippet 6: `NewType` and `List` Contravariance"
        },
        {
            "id": "newtype-list-contravariance",
            "metadata": "# EXPECTED:\n#   mypy: Error on `return get_raw_ids()`. Mypy correctly identifies that `List[int]` cannot be assigned to `List[TransactionId]` due to NewType's strictness and list invariance (for assignment), even though `int` is the base type of `TransactionId`.\n#   pyright: Error on `return get_raw_ids()`. Pyright also correctly enforces NewType and list type compatibility.\n#   pyre: No error. Pyre has historically been observed to be less strict about `NewType` in collection contexts, potentially allowing `List[int]` to be compatible with `List[TransactionId]` in return positions.\n#   zuban: Error. Aims for strict type safety with NewType.\n# REASON: Type checkers differ in their strictness when a `NewType` (which is a nominal subtype) is involved in generic collections. While `NewType` is a subtype of its base type (`int`), a collection of the base type (`List[int]`) is not type-compatible with a collection of the `NewType` (`List[TransactionId]`). This is a subtle contravariance/invariance rule that some checkers might relax.",
            "code": "from typing import NewType, List, Callable, TypeVar, Any\n\nTransactionId = NewType('TransactionId', int)\nItemId = NewType('ItemId', int)\n\ndef get_raw_ids() -> List[int]:\n    \"\"\"Simulates fetching raw integer IDs from a database.\"\"\"\n    print(\"Fetching raw integer IDs.\")\n    return [101, 102, 103, 104]\n\ndef fetch_transaction_ids() -> List[TransactionId]:\n    \"\"\"Function expected to return a list of TransactionId NewType.\"\"\"\n    # This line is the potential divergence point.\n    # It attempts to return List[int] where List[TransactionId] is expected.\n    # Mypy, Pyright, Zuban should flag this as an error. Pyre might not.\n    return get_raw_ids()\n\ndef process_ids(ids: List[TransactionId]) -> None:\n    \"\"\"Processes a list of TransactionIds.\"\"\"\n    print(f\"Processing IDs of type {type(ids[0]) if ids else 'empty'}: {ids}\")\n    # ids.append(200) # This should be a type error in all, as 200 is int, not TransactionId\n    # ids.append(TransactionId(200)) # This is fine\n\nif __name__ == \"__main__\":\n    print(\"--- Demonstrating correct usage ---\")\n    strict_transactions: List[TransactionId] = [TransactionId(1), TransactionId(2)]\n    process_ids(strict_transactions)\n\n    print(\"\\n--- Demonstrating divergence ---\")\n    # Calling the problematic function. The type checker divergence occurs here\n    # during the return statement's assignment check.\n    try_transactions = fetch_transaction_ids()\n    reveal_type(try_transactions) # If no error, this reveals List[TransactionId]\n    process_ids(try_transactions)\n\n---\n\n### Snippet 7: Overload with `Literal` Discrimination",
            "full_content": "# id: newtype-list-contravariance\n# EXPECTED:\n#   mypy: Error on `return get_raw_ids()`. Mypy correctly identifies that `List[int]` cannot be assigned to `List[TransactionId]` due to NewType's strictness and list invariance (for assignment), even though `int` is the base type of `TransactionId`.\n#   pyright: Error on `return get_raw_ids()`. Pyright also correctly enforces NewType and list type compatibility.\n#   pyre: No error. Pyre has historically been observed to be less strict about `NewType` in collection contexts, potentially allowing `List[int]` to be compatible with `List[TransactionId]` in return positions.\n#   zuban: Error. Aims for strict type safety with NewType.\n# REASON: Type checkers differ in their strictness when a `NewType` (which is a nominal subtype) is involved in generic collections. While `NewType` is a subtype of its base type (`int`), a collection of the base type (`List[int]`) is not type-compatible with a collection of the `NewType` (`List[TransactionId]`). This is a subtle contravariance/invariance rule that some checkers might relax.\n\nfrom typing import NewType, List, Callable, TypeVar, Any\n\nTransactionId = NewType('TransactionId', int)\nItemId = NewType('ItemId', int)\n\ndef get_raw_ids() -> List[int]:\n    \"\"\"Simulates fetching raw integer IDs from a database.\"\"\"\n    print(\"Fetching raw integer IDs.\")\n    return [101, 102, 103, 104]\n\ndef fetch_transaction_ids() -> List[TransactionId]:\n    \"\"\"Function expected to return a list of TransactionId NewType.\"\"\"\n    # This line is the potential divergence point.\n    # It attempts to return List[int] where List[TransactionId] is expected.\n    # Mypy, Pyright, Zuban should flag this as an error. Pyre might not.\n    return get_raw_ids()\n\ndef process_ids(ids: List[TransactionId]) -> None:\n    \"\"\"Processes a list of TransactionIds.\"\"\"\n    print(f\"Processing IDs of type {type(ids[0]) if ids else 'empty'}: {ids}\")\n    # ids.append(200) # This should be a type error in all, as 200 is int, not TransactionId\n    # ids.append(TransactionId(200)) # This is fine\n\nif __name__ == \"__main__\":\n    print(\"--- Demonstrating correct usage ---\")\n    strict_transactions: List[TransactionId] = [TransactionId(1), TransactionId(2)]\n    process_ids(strict_transactions)\n\n    print(\"\\n--- Demonstrating divergence ---\")\n    # Calling the problematic function. The type checker divergence occurs here\n    # during the return statement's assignment check.\n    try_transactions = fetch_transaction_ids()\n    reveal_type(try_transactions) # If no error, this reveals List[TransactionId]\n    process_ids(try_transactions)\n\n---\n\n### Snippet 7: Overload with `Literal` Discrimination"
        },
        {
            "id": "overload-literal-discrimination-union-fallback",
            "metadata": "# EXPECTED:\n#   mypy: `result1` -> `bool`, `result2` -> `int`, `result3` -> `float`, `result4` -> `str`. Correct discrimination.\n#   pyright: `result1` -> `bool`, `result2` -> `int`, `result3` -> `float`, `result4` -> `str`. Correct discrimination.\n#   pyre: `result1` -> `Union[bool, int, float, str]`, `result2` -> `Union[bool, int, float, str]`, etc. Pyre might fail to discriminate fully, especially with multiple Literal overloads and a catch-all fallback, leading to a wider union type.\n#   zuban: `result1` -> `bool`, `result2` -> `int`, `result3` -> `float`, `result4` -> `str`. Correct discrimination.\n# REASON: Type checkers differ in their ability to perform precise overload resolution, particularly when `Literal` types are used for discrimination alongside a general \"catch-all\" overload. Some checkers correctly identify the most specific matching overload for a given literal argument, while others might fall back to a wider union of all possible return types from all overloads, even for arguments that clearly match a specific `Literal`.",
            "code": "from typing import overload, Literal, Union\n\n@overload\ndef parse_config_value(value: Literal[\"true\", \"false\"]) -> bool: ...\n@overload\ndef parse_config_value(value: Literal[\"0\", \"1\", \"2\"]) -> int: ...\n@overload\ndef parse_config_value(value: Literal[\"3.14\", \"2.71\"]) -> float: ...\n@overload\ndef parse_config_value(value: str) -> str: ... # Catch-all for any other string\n\ndef parse_config_value(value: str) -> Union[bool, int, float, str]:\n    \"\"\"Parses a configuration string into a more specific type.\"\"\"\n    if value in (\"true\", \"false\"):\n        return value == \"true\"\n    if value in (\"0\", \"1\", \"2\"):\n        return int(value)\n    if value in (\"3.14\", \"2.71\"):\n        return float(value)\n    return value\n\nif __name__ == \"__main__\":\n    # Test literal string arguments\n    result1 = parse_config_value(\"true\")\n    reveal_type(result1) # EXPECTED: bool (pyre might show Union[bool, int, float, str])\n\n    result2 = parse_config_value(\"1\")\n    reveal_type(result2) # EXPECTED: int (pyre might show Union[bool, int, float, str])\n\n    result3 = parse_config_value(\"3.14\")\n    reveal_type(result3) # EXPECTED: float (pyre might show Union[bool, int, float, str])\n\n    result4 = parse_config_value(\"any_other_string\")\n    reveal_type(result4) # EXPECTED: str (all should agree, as it hits the fallback)\n\n    # Test with a variable of type str (should always resolve to the widest union)\n    my_str_var: str = \"dynamic_value\"\n    result_var = parse_config_value(my_str_var)\n    reveal_type(result_var) # EXPECTED: Union[bool, int, float, str] (all should agree here)\n\n---\n\n### Snippet 8: `Final` Attribute Override with Property",
            "full_content": "# id: overload-literal-discrimination-union-fallback\n# EXPECTED:\n#   mypy: `result1` -> `bool`, `result2` -> `int`, `result3` -> `float`, `result4` -> `str`. Correct discrimination.\n#   pyright: `result1` -> `bool`, `result2` -> `int`, `result3` -> `float`, `result4` -> `str`. Correct discrimination.\n#   pyre: `result1` -> `Union[bool, int, float, str]`, `result2` -> `Union[bool, int, float, str]`, etc. Pyre might fail to discriminate fully, especially with multiple Literal overloads and a catch-all fallback, leading to a wider union type.\n#   zuban: `result1` -> `bool`, `result2` -> `int`, `result3` -> `float`, `result4` -> `str`. Correct discrimination.\n# REASON: Type checkers differ in their ability to perform precise overload resolution, particularly when `Literal` types are used for discrimination alongside a general \"catch-all\" overload. Some checkers correctly identify the most specific matching overload for a given literal argument, while others might fall back to a wider union of all possible return types from all overloads, even for arguments that clearly match a specific `Literal`.\n\nfrom typing import overload, Literal, Union\n\n@overload\ndef parse_config_value(value: Literal[\"true\", \"false\"]) -> bool: ...\n@overload\ndef parse_config_value(value: Literal[\"0\", \"1\", \"2\"]) -> int: ...\n@overload\ndef parse_config_value(value: Literal[\"3.14\", \"2.71\"]) -> float: ...\n@overload\ndef parse_config_value(value: str) -> str: ... # Catch-all for any other string\n\ndef parse_config_value(value: str) -> Union[bool, int, float, str]:\n    \"\"\"Parses a configuration string into a more specific type.\"\"\"\n    if value in (\"true\", \"false\"):\n        return value == \"true\"\n    if value in (\"0\", \"1\", \"2\"):\n        return int(value)\n    if value in (\"3.14\", \"2.71\"):\n        return float(value)\n    return value\n\nif __name__ == \"__main__\":\n    # Test literal string arguments\n    result1 = parse_config_value(\"true\")\n    reveal_type(result1) # EXPECTED: bool (pyre might show Union[bool, int, float, str])\n\n    result2 = parse_config_value(\"1\")\n    reveal_type(result2) # EXPECTED: int (pyre might show Union[bool, int, float, str])\n\n    result3 = parse_config_value(\"3.14\")\n    reveal_type(result3) # EXPECTED: float (pyre might show Union[bool, int, float, str])\n\n    result4 = parse_config_value(\"any_other_string\")\n    reveal_type(result4) # EXPECTED: str (all should agree, as it hits the fallback)\n\n    # Test with a variable of type str (should always resolve to the widest union)\n    my_str_var: str = \"dynamic_value\"\n    result_var = parse_config_value(my_str_var)\n    reveal_type(result_var) # EXPECTED: Union[bool, int, float, str] (all should agree here)\n\n---\n\n### Snippet 8: `Final` Attribute Override with Property"
        },
        {
            "id": "final-attribute-override-property",
            "metadata": "# EXPECTED:\n#   mypy: Error on `DetailedAppConfig.MODE`. Mypy flags attempts to redefine `Final` attributes, even with properties.\n#   pyright: Error on `DetailedAppConfig.MODE`. Pyright is also strict about `Final` attributes and redefinition.\n#   pyre: No error. Pyre has been observed to be less strict about `Final` attributes in class hierarchies, sometimes allowing a property to override a `Final` field. It might treat the property as a different \"kind\" of member.\n#   zuban: Error. Strict `Final` enforcement, considering redefinition via a property as a violation.\n# REASON: Type checkers differ in their interpretation of `Final` attributes when a derived class attempts to override them with a property (or a method) of the same name. Some checkers strictly enforce that a `Final` member cannot be redefined in any form, while others might consider a property to be a distinct kind of member that doesn't constitute a direct override in the same way a field would, thereby allowing it.",
            "code": "from typing import Final\n\nclass BaseAppConfig:\n    \"\"\"Base class with a final configuration mode.\"\"\"\n    MODE: Final[str] = \"PRODUCTION\" # This is a final class attribute\n\nclass DetailedAppConfig(BaseAppConfig):\n    \"\"\"Derived class attempting to override MODE with a property.\"\"\"\n    @property\n    def MODE(self) -> str: # This property has the same name as the Final attribute in BaseAppConfig\n        print(\"Accessing DetailedAppConfig.MODE property\")\n        return \"DEVELOPMENT\"\n\nclass AnotherAppConfig(BaseAppConfig):\n    \"\"\"Derived class attempting to override MODE with a regular attribute.\"\"\"\n    MODE: str = \"STAGING\" # This should be an error for all strict checkers.\n\ndef get_app_mode(config: BaseAppConfig) -> str:\n    return config.MODE\n\nif __name__ == \"__main__\":\n    print(\"--- Testing DetailedAppConfig ---\")\n    detailed_config = DetailedAppConfig()\n    # The divergence occurs during the definition of DetailedAppConfig.MODE\n    # or when assigning an instance of it to a BaseAppConfig variable.\n    print(f\"Detailed config mode (direct): {detailed_config.MODE}\")\n    reveal_type(detailed_config.MODE) # Should be str\n\n    # Assigning to a base class type hint.\n    # If the property override is allowed, this might change behavior.\n    base_view_config: BaseAppConfig = detailed_config\n    print(f\"Detailed config mode (via BaseAppConfig): {base_view_config.MODE}\")\n    reveal_type(base_view_config.MODE) # Still str, but definition caused the issue.\n\n    print(\"\\n--- Testing AnotherAppConfig (expected error for all) ---\")\n    # This assignment should cause a type error for all strict checkers,\n    # as it's a direct redefinition of a Final class attribute.\n    # another_config: BaseAppConfig = AnotherAppConfig()\n    # print(f\"Another config mode: {another_config.MODE}\")\n\n---\n\n### Snippet 9: Complex Generic Bounds (TypeVar Nesting)",
            "full_content": "# id: final-attribute-override-property\n# EXPECTED:\n#   mypy: Error on `DetailedAppConfig.MODE`. Mypy flags attempts to redefine `Final` attributes, even with properties.\n#   pyright: Error on `DetailedAppConfig.MODE`. Pyright is also strict about `Final` attributes and redefinition.\n#   pyre: No error. Pyre has been observed to be less strict about `Final` attributes in class hierarchies, sometimes allowing a property to override a `Final` field. It might treat the property as a different \"kind\" of member.\n#   zuban: Error. Strict `Final` enforcement, considering redefinition via a property as a violation.\n# REASON: Type checkers differ in their interpretation of `Final` attributes when a derived class attempts to override them with a property (or a method) of the same name. Some checkers strictly enforce that a `Final` member cannot be redefined in any form, while others might consider a property to be a distinct kind of member that doesn't constitute a direct override in the same way a field would, thereby allowing it.\n\nfrom typing import Final\n\nclass BaseAppConfig:\n    \"\"\"Base class with a final configuration mode.\"\"\"\n    MODE: Final[str] = \"PRODUCTION\" # This is a final class attribute\n\nclass DetailedAppConfig(BaseAppConfig):\n    \"\"\"Derived class attempting to override MODE with a property.\"\"\"\n    @property\n    def MODE(self) -> str: # This property has the same name as the Final attribute in BaseAppConfig\n        print(\"Accessing DetailedAppConfig.MODE property\")\n        return \"DEVELOPMENT\"\n\nclass AnotherAppConfig(BaseAppConfig):\n    \"\"\"Derived class attempting to override MODE with a regular attribute.\"\"\"\n    MODE: str = \"STAGING\" # This should be an error for all strict checkers.\n\ndef get_app_mode(config: BaseAppConfig) -> str:\n    return config.MODE\n\nif __name__ == \"__main__\":\n    print(\"--- Testing DetailedAppConfig ---\")\n    detailed_config = DetailedAppConfig()\n    # The divergence occurs during the definition of DetailedAppConfig.MODE\n    # or when assigning an instance of it to a BaseAppConfig variable.\n    print(f\"Detailed config mode (direct): {detailed_config.MODE}\")\n    reveal_type(detailed_config.MODE) # Should be str\n\n    # Assigning to a base class type hint.\n    # If the property override is allowed, this might change behavior.\n    base_view_config: BaseAppConfig = detailed_config\n    print(f\"Detailed config mode (via BaseAppConfig): {base_view_config.MODE}\")\n    reveal_type(base_view_config.MODE) # Still str, but definition caused the issue.\n\n    print(\"\\n--- Testing AnotherAppConfig (expected error for all) ---\")\n    # This assignment should cause a type error for all strict checkers,\n    # as it's a direct redefinition of a Final class attribute.\n    # another_config: BaseAppConfig = AnotherAppConfig()\n    # print(f\"Another config mode: {another_config.MODE}\")\n\n---\n\n### Snippet 9: Complex Generic Bounds (TypeVar Nesting)"
        },
        {
            "id": "complex-generic-bounds-typevar-nesting",
            "metadata": "# EXPECTED:\n#   mypy: No error. Mypy's recent versions have improved significantly in handling complex generic bounds.\n#   pyright: No error. Pyright is generally robust in generic type inference and bound checking.\n#   pyre: Error on `process_data_source(db_source)` or `reveal_type(first_data)`. Pyre might struggle to correctly resolve `T_Source`'s bound (`DataSource[T_Data]`), leading to an error when an instance of a concrete generic class is passed.\n#   zuban: No error. Aims for advanced generic reasoning and correctness.\n# REASON: Type checkers vary in their ability to handle complex generic bounds, especially when a `TypeVar`'s bound is itself a generic type parameterized by another `TypeVar`. This can lead to difficulties in determining subtype compatibility when an instance of a concrete generic class is passed to a function expecting such a deeply-bound `TypeVar`.",
            "code": "from typing import TypeVar, Generic, List, Protocol\n\nclass BaseData:\n    \"\"\"Base for all data objects.\"\"\"\n    id: int = 0\n\nclass UserData(BaseData):\n    \"\"\"Specific user data.\"\"\"\n    name: str = \"anon\"\n\nclass ProductData(BaseData):\n    \"\"\"Specific product data.\"\"\"\n    sku: str = \"N/A\"\n\n# T_Data is bound to a base data type\nT_Data = TypeVar('T_Data', bound=BaseData)\n\n# DataSource is a generic class that manages a list of T_Data\nclass DataSource(Generic[T_Data]):\n    def __init__(self, data_items: List[T_Data]) -> None:\n        self.data_items = data_items\n\n    def get_first(self) -> T_Data:\n        return self.data_items[0]\n\n# T_Source is a TypeVar whose bound is a DataSource that manages *any* T_Data\n# This is the complex part: T_Source bound to a generic type that itself uses a TypeVar\nT_Source = TypeVar('T_Source', bound=DataSource[T_Data])\n\ndef process_data_source(source: T_Source) -> None:\n    \"\"\"Function that processes a data source bound by T_Source.\"\"\"\n    first_data = source.get_first()\n    print(f\"Processing source. First item ID: {first_data.id}\")\n    reveal_type(source)      # Expected: DataSource[UserData] or DataSource[ProductData]\n    reveal_type(first_data)  # Expected: UserData or ProductData\n\nif __name__ == \"__main__\":\n    user_data_list: List[UserData] = [UserData(id=1, name=\"Alice\"), UserData(id=2, name=\"Bob\")]\n    user_source = DataSource(user_data_list)\n\n    print(\"--- Processing UserData Source ---\")\n    # This is the critical call for divergence:\n    # Does `user_source` (DataSource[UserData]) fit the bound `T_Source` (bound=DataSource[T_Data])?\n    process_data_source(user_source) # Pyre might error here.\n\n    product_data_list: List[ProductData] = [ProductData(id=10, sku=\"A123\"), ProductData(id=11, sku=\"B456\")]\n    product_source = DataSource(product_data_list)\n\n    print(\"\\n--- Processing ProductData Source ---\")\n    process_data_source(product_source) # Pyre might error here too.\n\n---\n\n### Snippet 10: Protocol for `__call__` (Callable) with Positional vs. Keyword Arguments",
            "full_content": "# id: complex-generic-bounds-typevar-nesting\n# EXPECTED:\n#   mypy: No error. Mypy's recent versions have improved significantly in handling complex generic bounds.\n#   pyright: No error. Pyright is generally robust in generic type inference and bound checking.\n#   pyre: Error on `process_data_source(db_source)` or `reveal_type(first_data)`. Pyre might struggle to correctly resolve `T_Source`'s bound (`DataSource[T_Data]`), leading to an error when an instance of a concrete generic class is passed.\n#   zuban: No error. Aims for advanced generic reasoning and correctness.\n# REASON: Type checkers vary in their ability to handle complex generic bounds, especially when a `TypeVar`'s bound is itself a generic type parameterized by another `TypeVar`. This can lead to difficulties in determining subtype compatibility when an instance of a concrete generic class is passed to a function expecting such a deeply-bound `TypeVar`.\n\nfrom typing import TypeVar, Generic, List, Protocol\n\nclass BaseData:\n    \"\"\"Base for all data objects.\"\"\"\n    id: int = 0\n\nclass UserData(BaseData):\n    \"\"\"Specific user data.\"\"\"\n    name: str = \"anon\"\n\nclass ProductData(BaseData):\n    \"\"\"Specific product data.\"\"\"\n    sku: str = \"N/A\"\n\n# T_Data is bound to a base data type\nT_Data = TypeVar('T_Data', bound=BaseData)\n\n# DataSource is a generic class that manages a list of T_Data\nclass DataSource(Generic[T_Data]):\n    def __init__(self, data_items: List[T_Data]) -> None:\n        self.data_items = data_items\n\n    def get_first(self) -> T_Data:\n        return self.data_items[0]\n\n# T_Source is a TypeVar whose bound is a DataSource that manages *any* T_Data\n# This is the complex part: T_Source bound to a generic type that itself uses a TypeVar\nT_Source = TypeVar('T_Source', bound=DataSource[T_Data])\n\ndef process_data_source(source: T_Source) -> None:\n    \"\"\"Function that processes a data source bound by T_Source.\"\"\"\n    first_data = source.get_first()\n    print(f\"Processing source. First item ID: {first_data.id}\")\n    reveal_type(source)      # Expected: DataSource[UserData] or DataSource[ProductData]\n    reveal_type(first_data)  # Expected: UserData or ProductData\n\nif __name__ == \"__main__\":\n    user_data_list: List[UserData] = [UserData(id=1, name=\"Alice\"), UserData(id=2, name=\"Bob\")]\n    user_source = DataSource(user_data_list)\n\n    print(\"--- Processing UserData Source ---\")\n    # This is the critical call for divergence:\n    # Does `user_source` (DataSource[UserData]) fit the bound `T_Source` (bound=DataSource[T_Data])?\n    process_data_source(user_source) # Pyre might error here.\n\n    product_data_list: List[ProductData] = [ProductData(id=10, sku=\"A123\"), ProductData(id=11, sku=\"B456\")]\n    product_source = DataSource(product_data_list)\n\n    print(\"\\n--- Processing ProductData Source ---\")\n    process_data_source(product_source) # Pyre might error here too.\n\n---\n\n### Snippet 10: Protocol for `__call__` (Callable) with Positional vs. Keyword Arguments"
        },
        {
            "id": "protocol-callable-keyword-only-vs-positional-or-keyword",
            "metadata": "# EXPECTED:\n#   mypy: No error on `validator: DataValidator = positional_validator`. Mypy typically allows positional-or-keyword arguments to satisfy keyword-only requirements if names and types match.\n#   pyright: Error on `validator: DataValidator = positional_validator` and `validator: DataValidator = positional_no_default_validator`. Pyright is stricter about `keyword-only` in protocols, enforcing the `*` separator.\n#   pyre: Error on `validator: DataValidator = positional_validator` and `validator: DataValidator = positional_no_default_validator`. Pyre often aligns with stricter interpretations of callable compatibility.\n#   zuban: Error on `validator: DataValidator = positional_validator` and `validator: DataValidator = positional_no_default_validator`. Follows strict keyword-only enforcement.\n# REASON: Type checkers differ in their interpretation of compatibility between a `Protocol` requiring keyword-only arguments for its `__call__` method and a concrete function that defines those arguments as positional-or-keyword. Some consider this compatible as long as the types and names match, while others strictly enforce the keyword-only nature introduced by the `*` separator in the protocol's signature.",
            "code": "from typing import Protocol, Callable\n\nclass DataValidator(Protocol):\n    \"\"\"Protocol for a callable that validates data with keyword-only arguments.\"\"\"\n    def __call__(self, *, data: dict[str, str], strict: bool = True) -> bool: ...\n\ndef keyword_only_validator(*, data: dict[str, str], strict: bool = True) -> bool:\n    \"\"\"Function with explicit keyword-only arguments, matching the protocol.\"\"\"\n    print(f\"  Keyword-only validator called (strict={strict})\")\n    return \"value\" in data and (strict or \"optional\" not in data)\n\ndef positional_validator(data: dict[str, str], strict: bool = True) -> bool:\n    \"\"\"Function with positional-or-keyword arguments.\"\"\"\n    print(f\"  Positional-or-keyword validator called (strict={strict})\")\n    return \"value\" in data and (strict or \"optional\" not in data)\n\ndef positional_no_default_validator(data: dict[str, str], strict: bool) -> bool:\n    \"\"\"Function with positional-or-keyword arguments and no default for strict.\"\"\"\n    print(f\"  Positional (no default) validator called (strict={strict})\")\n    return \"value\" in data and (strict or \"optional\" not in data)\n\ndef use_validator(validator_func: DataValidator, payload: dict[str, str]) -> None:\n    result_default = validator_func(data=payload)\n    print(f\"    Validation result (default strict): {result_default}\")\n    result_strict = validator_func(data=payload, strict=False)\n    print(f\"    Validation result (strict=False): {result_strict}\")\n\nif __name__ == \"__main__\":\n    sample_data = {\"key\": \"value\", \"optional\": \"true\"}\n\n    print(\"--- Testing keyword_only_validator (all should pass) ---\")\n    validator_kw: DataValidator = keyword_only_validator\n    use_validator(validator_kw, sample_data)\n    reveal_type(validator_kw)\n\n    print(\"\\n--- Testing positional_validator (mypy passes, others diverge) ---\")\n    # This is the main divergence point:\n    # Mypy is expected to allow this, Pyright/Pyre/Zuban to flag an error.\n    validator_pos: DataValidator = positional_validator\n    use_validator(validator_pos, sample_data)\n    reveal_type(validator_pos)\n\n    print(\"\\n--- Testing positional_no_default_validator (most/all should fail) ---\")\n    # This should fail for Pyright/Pyre/Zuban due to keyword-only + missing default.\n    # Mypy *might* still allow it if it only checks callability and type, but the missing default\n    # for `strict` when the protocol expects one makes it less likely even for mypy.\n    validator_pos_no_default: DataValidator = positional_no_default_validator\n    use_validator(validator_pos_no_default, sample_data)\n    reveal_type(validator_pos_no_default)\n\n---\n\nI performed one comprehensive round of generating and refining these 10 examples, simulating the type checker behavior based on known divergences and typical strictness levels for each tool.",
            "full_content": "# id: protocol-callable-keyword-only-vs-positional-or-keyword\n# EXPECTED:\n#   mypy: No error on `validator: DataValidator = positional_validator`. Mypy typically allows positional-or-keyword arguments to satisfy keyword-only requirements if names and types match.\n#   pyright: Error on `validator: DataValidator = positional_validator` and `validator: DataValidator = positional_no_default_validator`. Pyright is stricter about `keyword-only` in protocols, enforcing the `*` separator.\n#   pyre: Error on `validator: DataValidator = positional_validator` and `validator: DataValidator = positional_no_default_validator`. Pyre often aligns with stricter interpretations of callable compatibility.\n#   zuban: Error on `validator: DataValidator = positional_validator` and `validator: DataValidator = positional_no_default_validator`. Follows strict keyword-only enforcement.\n# REASON: Type checkers differ in their interpretation of compatibility between a `Protocol` requiring keyword-only arguments for its `__call__` method and a concrete function that defines those arguments as positional-or-keyword. Some consider this compatible as long as the types and names match, while others strictly enforce the keyword-only nature introduced by the `*` separator in the protocol's signature.\n\nfrom typing import Protocol, Callable\n\nclass DataValidator(Protocol):\n    \"\"\"Protocol for a callable that validates data with keyword-only arguments.\"\"\"\n    def __call__(self, *, data: dict[str, str], strict: bool = True) -> bool: ...\n\ndef keyword_only_validator(*, data: dict[str, str], strict: bool = True) -> bool:\n    \"\"\"Function with explicit keyword-only arguments, matching the protocol.\"\"\"\n    print(f\"  Keyword-only validator called (strict={strict})\")\n    return \"value\" in data and (strict or \"optional\" not in data)\n\ndef positional_validator(data: dict[str, str], strict: bool = True) -> bool:\n    \"\"\"Function with positional-or-keyword arguments.\"\"\"\n    print(f\"  Positional-or-keyword validator called (strict={strict})\")\n    return \"value\" in data and (strict or \"optional\" not in data)\n\ndef positional_no_default_validator(data: dict[str, str], strict: bool) -> bool:\n    \"\"\"Function with positional-or-keyword arguments and no default for strict.\"\"\"\n    print(f\"  Positional (no default) validator called (strict={strict})\")\n    return \"value\" in data and (strict or \"optional\" not in data)\n\ndef use_validator(validator_func: DataValidator, payload: dict[str, str]) -> None:\n    result_default = validator_func(data=payload)\n    print(f\"    Validation result (default strict): {result_default}\")\n    result_strict = validator_func(data=payload, strict=False)\n    print(f\"    Validation result (strict=False): {result_strict}\")\n\nif __name__ == \"__main__\":\n    sample_data = {\"key\": \"value\", \"optional\": \"true\"}\n\n    print(\"--- Testing keyword_only_validator (all should pass) ---\")\n    validator_kw: DataValidator = keyword_only_validator\n    use_validator(validator_kw, sample_data)\n    reveal_type(validator_kw)\n\n    print(\"\\n--- Testing positional_validator (mypy passes, others diverge) ---\")\n    # This is the main divergence point:\n    # Mypy is expected to allow this, Pyright/Pyre/Zuban to flag an error.\n    validator_pos: DataValidator = positional_validator\n    use_validator(validator_pos, sample_data)\n    reveal_type(validator_pos)\n\n    print(\"\\n--- Testing positional_no_default_validator (most/all should fail) ---\")\n    # This should fail for Pyright/Pyre/Zuban due to keyword-only + missing default.\n    # Mypy *might* still allow it if it only checks callability and type, but the missing default\n    # for `strict` when the protocol expects one makes it less likely even for mypy.\n    validator_pos_no_default: DataValidator = positional_no_default_validator\n    use_validator(validator_pos_no_default, sample_data)\n    reveal_type(validator_pos_no_default)\n\n---\n\nI performed one comprehensive round of generating and refining these 10 examples, simulating the type checker behavior based on known divergences and typical strictness levels for each tool."
        }
    ]
}